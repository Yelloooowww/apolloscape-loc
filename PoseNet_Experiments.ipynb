{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoseNet Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from datasets.apolloscape import Apolloscape\n",
    "from utils.common import draw_poses\n",
    "from utils.common import draw_record\n",
    "from utils.common import imshow\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have original splits\n",
      "we have original splits\n",
      "Dataset: Apolloscape\n",
      "    Road: zpark-sample\n",
      "    Record: Record001\n",
      "    Train: True\n",
      "    Normalize Poses: True\n",
      "    Length: 121 of 1121\n",
      "    Cameras: ['Camera_2', 'Camera_1']\n",
      "    Records: ['Record001', 'Record002', 'Record003', 'Record004', 'Record006', 'Record007', 'Record008', 'Record009', 'Record010', 'Record011', 'Record012', 'Record013', 'Record014']\n",
      "\n",
      "Dataset: Apolloscape\n",
      "    Road: zpark-sample\n",
      "    Record: Record011\n",
      "    Train: False\n",
      "    Normalize Poses: True\n",
      "    Length: 87 of 378\n",
      "    Cameras: ['Camera_2', 'Camera_1']\n",
      "    Records: ['Record001', 'Record002', 'Record003', 'Record004', 'Record006', 'Record007', 'Record008', 'Record009', 'Record010', 'Record011', 'Record012', 'Record013', 'Record014']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "APOLLO_PATH = \"./data/apolloscape\"\n",
    "\n",
    "# Resize data before using\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(260),\n",
    "    transforms.CenterCrop(250),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# record_name = \"Record029\"\n",
    "# apollo_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"road02_seg\", transform=transform, record=record_name)\n",
    "\n",
    "# record_name = \"Record018\" # Record018 - example with a turn\n",
    "# apollo_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"road03_seg\", transform=transform, record=record_name)\n",
    "\n",
    "# record_name = \"Record008\"\n",
    "# apollo_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n",
    "#                              transform=transform, record=record_name, normalize_poses=True,\n",
    "#                              pose_format='quat')\n",
    "\n",
    "\n",
    "train_record = 'Record001'\n",
    "train_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n",
    "                             transform=transform, record=train_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=True)\n",
    "\n",
    "val_record = 'Record011'\n",
    "val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n",
    "                             transform=transform, record=val_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=False)\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8) # batch_size = 75\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8) # batch_size = 75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Records Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show records with numbers of data points\n",
    "recs_num = val_dataset.get_records_counts()\n",
    "recs_num = sorted(recs_num.items(), key=lambda kv: kv[0], reverse=True)\n",
    "print(\"Records Train:\")\n",
    "print(\"\\n\".join([\"\\t{} - {}\".format(r[0], r[1]) for r in recs_num ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_images(batch_samples):\n",
    "    just_images = [torch.cat(x, dim=2) for x in zip(*batch_samples[0])]\n",
    "    return just_images\n",
    "\n",
    "\n",
    "train_dataloader_iter = iter(train_dataloader)\n",
    "train_batch = next(train_dataloader_iter)\n",
    "\n",
    "print('len(batch) = {}'.format(len(train_batch)))\n",
    "print('len(batch[0]) = {}'.format(len(train_batch[0])))\n",
    "print('len(batch[0][0]) = {}'.format(len(train_batch[0][0])))\n",
    "\n",
    "print('batch_poses[0] = ', train_batch[1][0][0])\n",
    "pose = train_batch[1][0][0]\n",
    "npose = np.zeros(7)\n",
    "npose[:3] = pose.numpy()[:3] * train_dataset.poses_std[:3] + train_dataset.poses_mean[:3]\n",
    "npose[3:] = pose.numpy()[3:]\n",
    "pose = npose\n",
    "print('unnormalized pose = ', pose)\n",
    "\n",
    "images_col = collate_batch_images(train_batch)\n",
    "\n",
    "img_out = vutils.make_grid(images_col, nrow=1)\n",
    "imshow(img_out, title=\"Batch 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw datasets\n",
    "draw_record(train_dataset)\n",
    "# plt.title('Train')\n",
    "plt.show()\n",
    "\n",
    "draw_record(val_dataset)\n",
    "# plt.title('Val')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_extractor, num_features=128):\n",
    "        super(PoseNet, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        fc_in_features = self.feature_extractor.fc.in_features\n",
    "        self.feature_extractor.fc = torch.nn.Linear(fc_in_features, num_features)\n",
    "        \n",
    "        # Translation\n",
    "        self.fc_xyz = torch.nn.Linear(num_features, 3)\n",
    "        \n",
    "        # Rotation in quaternions\n",
    "        self.fc_quat = torch.nn.Linear(num_features, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is batch_images [2 x batch_size]\n",
    "        \n",
    "#         x = self.feature_extractor(x[0])\n",
    "\n",
    "#         for xi in x:\n",
    "#             print('type xi = {}'.format(type(xi)))\n",
    "        \n",
    "        x_features = [self.feature_extractor(xi) for xi in x]\n",
    "        x_translations = [self.fc_xyz(xi) for xi in x_features]\n",
    "        x_rotations = [self.fc_quat(xi) for xi in x_features]\n",
    "        \n",
    "        x_poses = [torch.cat((xt, xr), dim=1) for xt, xr in zip(x_translations, x_rotations)]\n",
    "        \n",
    "        return x_poses\n",
    "\n",
    "\n",
    "# Create model\n",
    "feature_extractor = models.resnet18(pretrained=True)\n",
    "model = PoseNet(feature_extractor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# feature_extractor = torch.nn.Sequential()\n",
    "# feature_extractor.add_module('conv1', torch.nn.Conv2d(in_channels=3, out_channels=128, kernel_size=4,\n",
    "#                     stride=2, padding=1, bias=True))\n",
    "# feature_extractor.add_module('relu1', torch.nn.LeakyReLU(0.2, inplace=False))\n",
    "# feature_extractor.add_module('relu1', torch.nn.LeakyReLU(0.2, inplace=False))\n",
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [tensor([[-0.2185, -0.4976,  0.5373, -0.5613,  0.4232, -0.0938, -0.0812],\n",
      "        [-0.2582, -0.5569,  0.4298, -0.5735,  0.4196,  0.0143,  0.0742],\n",
      "        [-0.3982, -0.4679,  0.4542, -0.5100, -0.0875, -0.2828, -0.3424],\n",
      "        [-0.1062, -0.6271,  0.1495, -0.6074,  0.3862,  0.0439, -0.1673],\n",
      "        [-0.4937, -0.3947,  0.3325, -0.6424,  0.1779, -0.2454, -0.0081],\n",
      "        [-0.1020, -0.5491,  0.3021, -0.8441,  0.3523,  0.3868, -0.0408],\n",
      "        [-0.5348, -0.5017,  0.4670, -0.7414,  0.4645, -0.0917,  0.0974],\n",
      "        [-0.4599, -0.2498,  0.7479, -0.3579,  0.2944,  0.2672, -0.1874]]), tensor([[ 0.0141, -0.5584,  0.3789, -0.6129,  0.3758, -0.2475,  0.1113],\n",
      "        [-0.1560, -0.0301,  0.7657, -0.4967,  0.2381,  0.0413, -0.0409],\n",
      "        [-0.6149, -0.2674,  0.4771, -0.8015,  0.0001,  0.1360, -0.2897],\n",
      "        [-0.2523, -0.5230,  0.5270, -0.3766,  0.0365, -0.0236, -0.3221],\n",
      "        [-0.4257, -0.8027,  0.0069, -0.6610,  0.6165,  0.0234,  0.3665],\n",
      "        [-0.2480, -0.4753,  0.2368, -0.6315,  0.2389,  0.0325, -0.0352],\n",
      "        [-0.3693, -0.6564,  0.5434, -0.5587,  0.3621,  0.1408, -0.3055],\n",
      "        [-0.4787, -0.6526,  0.5063, -0.6614,  0.4395, -0.0383, -0.1178]])]\n",
      "loss = 2.7442893981933594\n"
     ]
    }
   ],
   "source": [
    "# test_imgs1 = torch.rand(10, 3, 250, 250)\n",
    "# test_imgs2 = torch.rand(10, 3, 250, 250)\n",
    "batch_images = [torch.rand(8, 3, 250, 250) for _ in range(2)]\n",
    "batch_poses = [torch.rand(8, 7) for _ in range(2)]\n",
    "\n",
    "# for idx, m in enumerate(feature_extractor.modules()):\n",
    "#     print(\"{} -> {}\".format(idx, m))\n",
    "# print('out.len = {}'.format(len(out)))\n",
    "# print('out[0].len = {}'.format(len(out[0])))\n",
    "# batch_poses\n",
    "\n",
    "class PoseNetCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseNetCriterion, self).__init__()\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: list(N x 7, N x 7) - prediction (xyz, quat)\n",
    "            y: list(N x 7, N x 7) - target (xyz, quat)\n",
    "        \"\"\"\n",
    "#         print('x = {}'.format(x))\n",
    "#         print('y = {}'.format(y))\n",
    "        loss = 0\n",
    "        for i in range(2):\n",
    "#             print('part 1: {}; {}'.format(x[i][:, :3], y[i][:, :3]))\n",
    "#             print('part 2: {}; {}'.format(x[i][:, 3:], y[i][:, 3:]))\n",
    "            loss += self.loss_fn(x[i][:, :3], y[i][:, :3])\n",
    "            loss += self.loss_fn(x[i][:, 3:], y[i][:, 3:])\n",
    "        return loss\n",
    "    \n",
    "\n",
    "out = model(batch_images)\n",
    "print('out = {}'.format(out))\n",
    "\n",
    "criterion = PoseNetCriterion()\n",
    "loss = criterion(out, batch_poses)\n",
    "print('loss = {}'.format(loss))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "# batch_poses[0][:, :3, 3].size()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, value, n=1):\n",
    "        self.val = value\n",
    "        self.count += n\n",
    "        self.sum += value * n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "# train function\n",
    "def train(train_loader, model, criterion, optimizer, epoch, max_epoch):\n",
    "    \n",
    "    # switch model to training\n",
    "    model.train()\n",
    "    \n",
    "    log_freq = 5\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    for idx, (batch_images, batch_poses) in enumerate(train_loader):\n",
    "        data_time = (time.time() - end)\n",
    "        \n",
    "        batch_images = [x.to(device) for x in batch_images]\n",
    "        batch_poses = [x.to(device) for x in batch_poses]\n",
    "        \n",
    "        out = model(batch_images)\n",
    "        loss = criterion(out, batch_poses)\n",
    "\n",
    "        \n",
    "        losses.update(loss, len(batch_images) * batch_images[0].size(0))\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time = (time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if log_freq != 0 and idx % log_freq == 0:\n",
    "            print('Epoch: [{}/{}]\\tBatch: [{}/{}]\\t'\n",
    "                  'Time: {batch_time:.3f}\\t'\n",
    "                  'Data Time: {data_time:.3f}\\t'\n",
    "                  'Loss: {losses.val:.3f}\\t'\n",
    "                  'Avg Loss: {losses.avg:.3f}\\t'.format(\n",
    "                   epoch + 1, max_epoch, idx + 1, len(train_loader),\n",
    "                   batch_time=batch_time, data_time=data_time, losses=losses))\n",
    "            \n",
    "    print('Epoch: [{}/{}]\\tTraining Loss: {:.3f}'.format(epoch+1, max_epoch, losses.avg))\n",
    "    \n",
    "    \n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \n",
    "    \n",
    "    log_freq = 0 # len(val_loader)\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    # set model to evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (batch_images, batch_poses) in enumerate(val_loader):\n",
    "            data_time = time.time() - end\n",
    "            batch_images = [x.to(device) for x in batch_images]\n",
    "            batch_poses = [x.to(device) for x in batch_poses]\n",
    "            \n",
    "            # compute model output\n",
    "            out = model(batch_images)\n",
    "            loss = criterion(out, batch_poses)\n",
    "            \n",
    "            losses.update(loss, len(batch_images) * batch_images[0].size(0))\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            if log_freq != 0 and idx % log_freq == 0:\n",
    "                print('Val Epoch: {}\\t'\n",
    "                      'Time: {batch_time:.3f}\\t'\n",
    "                      'Data Time: {data_time:.3f}\\t'\n",
    "                      'Loss: {losses.val:.3f}\\t'\n",
    "                      'Avg Loss: {losses.avg:.3f}'.format(\n",
    "                       epoch + 1, batch_time=batch_time, data_time=data_time, losses=losses))\n",
    "                \n",
    "    print('Epoch: [{}]\\tValidation Loss: {:.3f}'.format(epoch+1, losses.avg))\n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "model = PoseNet(feature_extractor)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: _checkpoints/20180727_162918_ttt_e101.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Restore from checkpoint\n",
    "checkpoint_file = '_checkpoints/20180727_163351_ttt_e102.pth.tar'\n",
    "\n",
    "if 'checkpoint_file' in locals() and checkpoint_file is not None:\n",
    "    if os.path.isfile(checkpoint_file):\n",
    "        print('Loading from checkpoint: {}'.format(checkpoint_file))\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        \n",
    "\n",
    "# if 'start_epoch' not in locals():\n",
    "#     start_epoch = 0\n",
    "# n_epochs = start_epoch + 1\n",
    "# print('Epochs {} - {}'.format(start_epoch, n_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch: [102/102]\tBatch: [1/16]\tTime: 1.952\tData Time: 1.924\tLoss: 1.814\tAvg Loss: 1.814\t\n",
      "Epoch: [102/102]\tBatch: [6/16]\tTime: 1.917\tData Time: 1.895\tLoss: 0.544\tAvg Loss: 1.021\t\n",
      "Epoch: [102/102]\tBatch: [11/16]\tTime: 1.913\tData Time: 1.891\tLoss: 0.312\tAvg Loss: 0.800\t\n",
      "Epoch: [102/102]\tBatch: [16/16]\tTime: 0.231\tData Time: 0.219\tLoss: 0.620\tAvg Loss: 0.789\t\n",
      "Epoch: [102/102]\tTraining Loss: 0.789\n",
      "Epoch: [102]\tValidation Loss: 1.891\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'start_epoch' not in locals():\n",
    "    start_epoch = 0\n",
    "\n",
    "n_epochs = start_epoch + 1\n",
    "print('Training ...')\n",
    "for e in range(start_epoch, n_epochs):\n",
    "    \n",
    "    train(train_dataloader, model, criterion, optimizer, e, n_epochs)\n",
    "    \n",
    "#     print('Validation ...')\n",
    "    validate(val_dataloader, model, criterion, e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in feature_extractor.parameters():\n",
    "#     print(p.size())\n",
    "\n",
    "# Test function for visiting backward graph\n",
    "visited = set()\n",
    "def visit_all(var, indent=''):\n",
    "    if hasattr(var, 'data'):\n",
    "        print(\"{}Data: {}\".format(indent, var.data.size()))\n",
    "    if hasattr(var, 'grad'):\n",
    "        print(\"{}Grad Data: {}\".format(indent, var.grad))\n",
    "    if torch.is_tensor(var):\n",
    "        print(\"{}Tensor {}\".format(indent, var.size()))\n",
    "    else:\n",
    "        print(\"{}Type: {}\".format(indent, type(var)))\n",
    "\n",
    "    if hasattr(var, 'variable'):\n",
    "        print(\"{}Variable: {}\".format(indent, var.variable.size()))\n",
    "        print(\"{}Data: {}\".format(indent, var.variable))\n",
    "        print(\"{}Data GRAD: {}\".format(indent, var.variable.grad))\n",
    "    if hasattr(var, 'saved_tensors'):\n",
    "        print('{}Saved tensors: {}'.format(indent, var.saved_tensors))\n",
    "    if hasattr(var, 'next_functions'):\n",
    "        print('{}Next functions: {}'.format(indent, var.next_functions))\n",
    "        for f in var.next_functions:\n",
    "            if f[0] is not None:\n",
    "                print(\"{}Function: {}\".format(indent, f))\n",
    "                visit_all(f[0], indent=indent+'  ')\n",
    "    if hasattr(var, 'grad_fn'):\n",
    "        print('{}Grad Fn: {}'.format(indent, var.grad_fn))\n",
    "        visit_all(var.grad_fn, indent=indent+'  ')\n",
    "    visited.add(var)\n",
    "    \n",
    "    \n",
    "# visit_all(y)\n",
    "\n",
    "# print(out.grad_fn.next_functions)\n",
    "\n",
    "# out.__dir__()\n",
    "# make_dot(out, params=dict(feature_extractor.named_parameters()))\n",
    "make_dot(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple experiment to check backward graph state\n",
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "c = torch.rand(1, requires_grad=True)\n",
    "y = torch.log(a) * (b + torch.pow(c, 3))\n",
    "print('a = {}'.format(a))\n",
    "print('b = {}'.format(b))\n",
    "print('c = {}'.format(c))\n",
    "print('y = {}'.format(y))\n",
    "\n",
    "# visit_all(y)\n",
    "# print('--- BACKWARD ---')\n",
    "# y.backward()\n",
    "# visit_all(y)\n",
    "\n",
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to _checkpoints/20180727_163351_ttt_e102.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint\n",
    "def save_checkpoint(model, optimizer, experiment_name='test', epoch=None):\n",
    "    tstr = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    fname = '{}_{}'.format(tstr, experiment_name)\n",
    "    if epoch is not None:\n",
    "        fname += '_e{:03d}'.format(epoch)\n",
    "    fname += '.pth.tar'\n",
    "    \n",
    "    checkpoints_dir = '_checkpoints'\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "    \n",
    "    fname_path = os.path.join(checkpoints_dir, fname)\n",
    "#     print('fname_path = {}'.format(fname_path))\n",
    "    \n",
    "    checkpoint_dict = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optim_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint_dict, fname_path)\n",
    "    \n",
    "    print('Model saved to {}'.format(fname_path))\n",
    "    \n",
    "\n",
    "save_checkpoint(model, optimizer, 'ttt', n_epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
