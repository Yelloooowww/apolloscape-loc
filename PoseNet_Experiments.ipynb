{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoseNet Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from datasets.apolloscape import Apolloscape\n",
    "from utils.common import draw_poses\n",
    "from utils.common import draw_record\n",
    "from utils.common import imshow\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Apolloscape\n",
      "    Road: road03_seg\n",
      "    Record: None\n",
      "    Train: True\n",
      "    Normalize Poses: True\n",
      "    Length: 10956 of 10956\n",
      "    Cameras: ['Camera 5', 'Camera 6']\n",
      "    Records: ['Record007', 'Record008', 'Record009', 'Record010', 'Record011', 'Record012', 'Record013', 'Record014', 'Record015', 'Record016', 'Record017', 'Record018', 'Record020', 'Record036', 'Record037', 'Record038', 'Record039', 'Record040', 'Record041', 'Record042', 'Record043', 'Record044', 'Record045', 'Record046', 'Record047', 'Record048', 'Record049', 'Record050', 'Record051', 'Record052', 'Record053', 'Record054', 'Record055', 'Record056', 'Record057']\n",
      "\n",
      "Dataset: Apolloscape\n",
      "    Road: road03_seg\n",
      "    Record: None\n",
      "    Train: False\n",
      "    Normalize Poses: True\n",
      "    Length: 3653 of 3653\n",
      "    Cameras: ['Camera 5', 'Camera 6']\n",
      "    Records: ['Record007', 'Record008', 'Record009', 'Record010', 'Record011', 'Record012', 'Record013', 'Record014', 'Record015', 'Record016', 'Record017', 'Record018', 'Record020', 'Record036', 'Record037', 'Record038', 'Record039', 'Record040', 'Record041', 'Record042', 'Record043', 'Record044', 'Record045', 'Record046', 'Record047', 'Record048', 'Record049', 'Record050', 'Record051', 'Record052', 'Record053', 'Record054', 'Record055', 'Record056', 'Record057']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "APOLLO_PATH = \"./data/apolloscape\"\n",
    "\n",
    "# Resize data before using\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(260),\n",
    "    transforms.CenterCrop(250),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# record_name = \"Record029\"\n",
    "# apollo_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"road02_seg\", transform=transform, record=record_name)\n",
    "\n",
    "# record_name = \"Record018\" # Record018 - example with a turn\n",
    "# apollo_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"road03_seg\", transform=transform, record=record_name)\n",
    "\n",
    "# record_name = \"Record008\"\n",
    "# apollo_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n",
    "#                              transform=transform, record=record_name, normalize_poses=True,\n",
    "#                              pose_format='quat')\n",
    "\n",
    "\n",
    "# train_record = None # 'Record001'\n",
    "# train_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n",
    "#                              transform=transform, record=train_record, normalize_poses=True,\n",
    "#                              pose_format='quat', train=True, cache_transform=True)\n",
    "# val_record = None # 'Record011'\n",
    "# val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n",
    "#                              transform=transform, record=val_record, normalize_poses=True,\n",
    "#                              pose_format='quat', train=False, cache_transform=True)\n",
    "\n",
    "\n",
    "train_record = None # 'Record001'\n",
    "train_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"road03_seg\",\n",
    "                             transform=transform, record=train_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=True, cache_transform=True)\n",
    "val_record = None # 'Record011'\n",
    "val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"road03_seg\",\n",
    "                             transform=transform, record=val_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=False, cache_transform=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=75) # batch_size = 75\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=75) # batch_size = 75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Records Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Val:\n",
      "\tRecord057 - 192\n",
      "\tRecord056 - 1232\n",
      "\tRecord055 - 841\n",
      "\tRecord054 - 201\n",
      "\tRecord053 - 982\n",
      "\tRecord052 - 205\n",
      "\tRecord051 - 0\n",
      "\tRecord050 - 0\n",
      "\tRecord049 - 0\n",
      "\tRecord048 - 0\n",
      "\tRecord047 - 0\n",
      "\tRecord046 - 0\n",
      "\tRecord045 - 0\n",
      "\tRecord044 - 0\n",
      "\tRecord043 - 0\n",
      "\tRecord042 - 0\n",
      "\tRecord041 - 0\n",
      "\tRecord040 - 0\n",
      "\tRecord039 - 0\n",
      "\tRecord038 - 0\n",
      "\tRecord037 - 0\n",
      "\tRecord036 - 0\n",
      "\tRecord020 - 0\n",
      "\tRecord018 - 0\n",
      "\tRecord017 - 0\n",
      "\tRecord016 - 0\n",
      "\tRecord015 - 0\n",
      "\tRecord014 - 0\n",
      "\tRecord013 - 0\n",
      "\tRecord012 - 0\n",
      "\tRecord011 - 0\n",
      "\tRecord010 - 0\n",
      "\tRecord009 - 0\n",
      "\tRecord008 - 0\n",
      "\tRecord007 - 0\n"
     ]
    }
   ],
   "source": [
    "# Show records with numbers of data points\n",
    "recs_num = val_dataset.get_records_counts()\n",
    "recs_num = sorted(recs_num.items(), key=lambda kv: kv[0], reverse=True)\n",
    "print(\"Records Val:\")\n",
    "print(\"\\n\".join([\"\\t{} - {}\".format(r[0], r[1]) for r in recs_num ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_images(batch_samples):\n",
    "    just_images = [torch.cat(x, dim=2) for x in zip(*batch_samples[0])]\n",
    "    return just_images\n",
    "\n",
    "\n",
    "train_dataloader_iter = iter(train_dataloader)\n",
    "train_batch = next(train_dataloader_iter)\n",
    "\n",
    "print('len(batch) = {}'.format(len(train_batch)))\n",
    "print('len(batch[0]) = {}'.format(len(train_batch[0])))\n",
    "print('len(batch[0][0]) = {}'.format(len(train_batch[0][0])))\n",
    "\n",
    "print('batch_poses[0] = ', train_batch[1][0][0])\n",
    "pose = train_batch[1][0][0]\n",
    "npose = np.zeros(7)\n",
    "npose[:3] = pose.numpy()[:3] * train_dataset.poses_std[:3] + train_dataset.poses_mean[:3]\n",
    "npose[3:] = pose.numpy()[3:]\n",
    "pose = npose\n",
    "print('unnormalized pose = ', pose)\n",
    "\n",
    "images_col = collate_batch_images(train_batch)\n",
    "\n",
    "img_out = vutils.make_grid(images_col, nrow=1)\n",
    "imshow(img_out, title=\"Batch 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Train and Val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw datasets\n",
    "draw_record(train_dataset)\n",
    "# plt.title('Train')\n",
    "plt.show()\n",
    "\n",
    "draw_record(val_dataset)\n",
    "# plt.title('Val')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_extractor, num_features=128):\n",
    "        super(PoseNet, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        fc_in_features = self.feature_extractor.fc.in_features\n",
    "        self.feature_extractor.fc = torch.nn.Linear(fc_in_features, num_features)\n",
    "        \n",
    "        # Translation\n",
    "        self.fc_xyz = torch.nn.Linear(num_features, 3)\n",
    "        \n",
    "        # Rotation in quaternions\n",
    "        self.fc_quat = torch.nn.Linear(num_features, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is batch_images [2 x batch_size]\n",
    "        \n",
    "#         x = self.feature_extractor(x[0])\n",
    "\n",
    "#         for xi in x:\n",
    "#             print('type xi = {}'.format(type(xi)))\n",
    "        \n",
    "        x_features = [self.feature_extractor(xi) for xi in x]\n",
    "        x_translations = [self.fc_xyz(xi) for xi in x_features]\n",
    "        x_rotations = [self.fc_quat(xi) for xi in x_features]\n",
    "        \n",
    "        x_poses = [torch.cat((xt, xr), dim=1) for xt, xr in zip(x_translations, x_rotations)]\n",
    "        \n",
    "        return x_poses\n",
    "\n",
    "\n",
    "# Create model\n",
    "feature_extractor = models.resnet18(pretrained=True)\n",
    "model = PoseNet(feature_extractor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)\n",
    "\n",
    "# feature_extractor = torch.nn.Sequential()\n",
    "# feature_extractor.add_module('conv1', torch.nn.Conv2d(in_channels=3, out_channels=128, kernel_size=4,\n",
    "#                     stride=2, padding=1, bias=True))\n",
    "# feature_extractor.add_module('relu1', torch.nn.LeakyReLU(0.2, inplace=False))\n",
    "# feature_extractor.add_module('relu1', torch.nn.LeakyReLU(0.2, inplace=False))\n",
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoseNetCriterion and sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [tensor([[ 0.1892,  0.3551, -0.3748, -0.4954, -0.3807, -0.2712, -0.1940]]), tensor([[ 0.1945,  0.3340, -0.3670, -0.5002, -0.3890, -0.2326, -0.1986]])]\n",
      "loss = 2.572418212890625\n"
     ]
    }
   ],
   "source": [
    "# test_imgs1 = torch.rand(10, 3, 250, 250)\n",
    "# test_imgs2 = torch.rand(10, 3, 250, 250)\n",
    "batch_images = [torch.rand(1, 3, 250, 250) for _ in range(2)]\n",
    "batch_poses = [torch.rand(1, 7) for _ in range(2)]\n",
    "\n",
    "# for idx, m in enumerate(feature_extractor.modules()):\n",
    "#     print(\"{} -> {}\".format(idx, m))\n",
    "# print('out.len = {}'.format(len(out)))\n",
    "# print('out[0].len = {}'.format(len(out[0])))\n",
    "# batch_poses\n",
    "\n",
    "class PoseNetCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseNetCriterion, self).__init__()\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: list(N x 7, N x 7) - prediction (xyz, quat)\n",
    "            y: list(N x 7, N x 7) - target (xyz, quat)\n",
    "        \"\"\"\n",
    "#         print('x = {}'.format(x))\n",
    "#         print('y = {}'.format(y))\n",
    "        loss = 0\n",
    "        for i in range(2):\n",
    "#             print('part 1: {}; {}'.format(x[i][:, :3], y[i][:, :3]))\n",
    "#             print('part 2: {}; {}'.format(x[i][:, 3:], y[i][:, 3:]))\n",
    "            loss += self.loss_fn(x[i][:, :3], y[i][:, :3])\n",
    "            loss += self.loss_fn(x[i][:, 3:], y[i][:, 3:])\n",
    "        return loss\n",
    "    \n",
    "\n",
    "out = model(batch_images)\n",
    "print('out = {}'.format(out))\n",
    "\n",
    "criterion = PoseNetCriterion()\n",
    "loss = criterion(out, batch_poses)\n",
    "print('loss = {}'.format(loss))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "# batch_poses[0][:, :3, 3].size()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, value, n=1):\n",
    "        self.val = value\n",
    "        self.count += n\n",
    "        self.sum += value * n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "# train function\n",
    "def train(train_loader, model, criterion, optimizer, epoch, max_epoch):\n",
    "    \n",
    "    # switch model to training\n",
    "    model.train()\n",
    "    \n",
    "    log_freq = 5\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    for idx, (batch_images, batch_poses) in enumerate(train_loader):\n",
    "#         if idx < len(train_loader) - 1: continue\n",
    "        data_time = (time.time() - end)\n",
    "        \n",
    "        batch_images = [x.to(device) for x in batch_images]\n",
    "        batch_poses = [x.to(device) for x in batch_poses]\n",
    "        \n",
    "        out = model(batch_images)\n",
    "        loss = criterion(out, batch_poses)\n",
    "\n",
    "        \n",
    "        losses.update(loss, len(batch_images) * batch_images[0].size(0))\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time = (time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if log_freq != 0 and idx % log_freq == 0:\n",
    "            print('Epoch: [{}/{}]\\tBatch: [{}/{}]\\t'\n",
    "                  'Time: {batch_time:.3f}\\t'\n",
    "                  'Data Time: {data_time:.3f}\\t'\n",
    "                  'Loss: {losses.val:.3f}\\t'\n",
    "                  'Avg Loss: {losses.avg:.3f}\\t'.format(\n",
    "                   epoch + 1, max_epoch, idx + 1, len(train_loader),\n",
    "                   batch_time=batch_time, data_time=data_time, losses=losses))\n",
    "            \n",
    "    print('Epoch: [{}/{}]\\tTraining Loss: {:.3f}'.format(epoch+1, max_epoch, losses.avg))\n",
    "    \n",
    "    \n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \n",
    "    \n",
    "    log_freq = 0 # len(val_loader)\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    # set model to evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (batch_images, batch_poses) in enumerate(val_loader):\n",
    "            data_time = time.time() - end\n",
    "            batch_images = [x.to(device) for x in batch_images]\n",
    "            batch_poses = [x.to(device) for x in batch_poses]\n",
    "            \n",
    "            # compute model output\n",
    "            out = model(batch_images)\n",
    "            loss = criterion(out, batch_poses)\n",
    "            \n",
    "            losses.update(loss, len(batch_images) * batch_images[0].size(0))\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            if log_freq != 0 and idx % log_freq == 0:\n",
    "                print('Val Epoch: {}\\t'\n",
    "                      'Time: {batch_time:.3f}\\t'\n",
    "                      'Data Time: {data_time:.3f}\\t'\n",
    "                      'Loss: {losses.val:.3f}\\t'\n",
    "                      'Avg Loss: {losses.avg:.3f}'.format(\n",
    "                       epoch + 1, batch_time=batch_time, data_time=data_time, losses=losses))\n",
    "                \n",
    "    print('Epoch: [{}]\\tValidation Loss: {:.3f}'.format(epoch+1, losses.avg))\n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh model & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "model = PoseNet(feature_extractor)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: _checkpoints/20180728_124631_road02_128_e001.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Restore from checkpoint\n",
    "checkpoint_file = '_checkpoints/20180728_125439_road02_128_e002.pth.tar'\n",
    "\n",
    "if 'checkpoint_file' in locals() and checkpoint_file is not None:\n",
    "    if os.path.isfile(checkpoint_file):\n",
    "        print('Loading from checkpoint: {}'.format(checkpoint_file))\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        \n",
    "\n",
    "# if 'start_epoch' not in locals():\n",
    "#     start_epoch = 0\n",
    "# n_epochs = start_epoch + 1\n",
    "# print('Epochs {} - {}'.format(start_epoch, n_epochs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch: [2/2]\tBatch: [1/147]\tTime: 1.176\tData Time: 1.048\tLoss: 0.568\tAvg Loss: 0.568\t\n",
      "Epoch: [2/2]\tBatch: [6/147]\tTime: 1.195\tData Time: 1.040\tLoss: 1.485\tAvg Loss: 1.093\t\n",
      "Epoch: [2/2]\tBatch: [11/147]\tTime: 1.156\tData Time: 1.036\tLoss: 2.850\tAvg Loss: 1.618\t\n",
      "Epoch: [2/2]\tBatch: [16/147]\tTime: 1.205\tData Time: 1.051\tLoss: 4.288\tAvg Loss: 2.258\t\n",
      "Epoch: [2/2]\tBatch: [21/147]\tTime: 1.145\tData Time: 1.025\tLoss: 3.227\tAvg Loss: 2.574\t\n",
      "Epoch: [2/2]\tBatch: [26/147]\tTime: 1.161\tData Time: 1.041\tLoss: 3.544\tAvg Loss: 2.749\t\n",
      "Epoch: [2/2]\tBatch: [31/147]\tTime: 1.153\tData Time: 1.034\tLoss: 1.669\tAvg Loss: 2.648\t\n",
      "Epoch: [2/2]\tBatch: [36/147]\tTime: 1.164\tData Time: 1.030\tLoss: 1.691\tAvg Loss: 2.536\t\n",
      "Epoch: [2/2]\tBatch: [41/147]\tTime: 1.143\tData Time: 1.034\tLoss: 2.201\tAvg Loss: 2.492\t\n",
      "Epoch: [2/2]\tBatch: [46/147]\tTime: 1.180\tData Time: 1.028\tLoss: 3.633\tAvg Loss: 2.538\t\n",
      "Epoch: [2/2]\tBatch: [51/147]\tTime: 1.155\tData Time: 1.014\tLoss: 2.710\tAvg Loss: 2.585\t\n",
      "Epoch: [2/2]\tBatch: [56/147]\tTime: 1.183\tData Time: 1.016\tLoss: 3.541\tAvg Loss: 2.636\t\n",
      "Epoch: [2/2]\tBatch: [61/147]\tTime: 1.152\tData Time: 1.028\tLoss: 1.992\tAvg Loss: 2.616\t\n",
      "Epoch: [2/2]\tBatch: [66/147]\tTime: 1.136\tData Time: 1.014\tLoss: 2.441\tAvg Loss: 2.599\t\n",
      "Epoch: [2/2]\tBatch: [71/147]\tTime: 1.164\tData Time: 1.014\tLoss: 2.449\tAvg Loss: 2.618\t\n",
      "Epoch: [2/2]\tBatch: [76/147]\tTime: 1.141\tData Time: 1.020\tLoss: 2.580\tAvg Loss: 2.645\t\n",
      "Epoch: [2/2]\tBatch: [81/147]\tTime: 1.177\tData Time: 1.028\tLoss: 2.190\tAvg Loss: 2.640\t\n",
      "Epoch: [2/2]\tBatch: [86/147]\tTime: 1.131\tData Time: 1.009\tLoss: 2.287\tAvg Loss: 2.635\t\n",
      "Epoch: [2/2]\tBatch: [91/147]\tTime: 1.160\tData Time: 1.014\tLoss: 2.213\tAvg Loss: 2.597\t\n",
      "Epoch: [2/2]\tBatch: [96/147]\tTime: 1.123\tData Time: 0.996\tLoss: 2.931\tAvg Loss: 2.588\t\n",
      "Epoch: [2/2]\tBatch: [101/147]\tTime: 1.125\tData Time: 0.996\tLoss: 2.978\tAvg Loss: 2.613\t\n",
      "Epoch: [2/2]\tBatch: [106/147]\tTime: 1.126\tData Time: 1.004\tLoss: 2.628\tAvg Loss: 2.603\t\n",
      "Epoch: [2/2]\tBatch: [111/147]\tTime: 1.140\tData Time: 1.006\tLoss: 2.721\tAvg Loss: 2.612\t\n",
      "Epoch: [2/2]\tBatch: [116/147]\tTime: 1.107\tData Time: 0.985\tLoss: 2.930\tAvg Loss: 2.604\t\n",
      "Epoch: [2/2]\tBatch: [121/147]\tTime: 1.109\tData Time: 1.000\tLoss: 2.836\tAvg Loss: 2.597\t\n",
      "Epoch: [2/2]\tBatch: [126/147]\tTime: 1.114\tData Time: 0.993\tLoss: 2.762\tAvg Loss: 2.610\t\n",
      "Epoch: [2/2]\tBatch: [131/147]\tTime: 1.118\tData Time: 0.995\tLoss: 2.845\tAvg Loss: 2.615\t\n",
      "Epoch: [2/2]\tBatch: [136/147]\tTime: 1.104\tData Time: 0.983\tLoss: 2.791\tAvg Loss: 2.610\t\n",
      "Epoch: [2/2]\tBatch: [141/147]\tTime: 1.160\tData Time: 1.016\tLoss: 2.761\tAvg Loss: 2.623\t\n",
      "Epoch: [2/2]\tBatch: [146/147]\tTime: 1.131\tData Time: 1.010\tLoss: 2.928\tAvg Loss: 2.614\t\n",
      "Epoch: [2/2]\tTraining Loss: 2.614\n",
      "Epoch: [2]\tValidation Loss: 2.402\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'start_epoch' not in locals():\n",
    "    start_epoch = 0\n",
    "\n",
    "n_epochs = start_epoch + 1\n",
    "print('Training ...')\n",
    "for e in range(start_epoch, n_epochs):\n",
    "    \n",
    "    train(train_dataloader, model, criterion, optimizer, e, n_epochs)\n",
    "    \n",
    "#     print('Validation ...')\n",
    "    validate(val_dataloader, model, criterion, e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize PoseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in feature_extractor.parameters():\n",
    "#     print(p.size())\n",
    "\n",
    "# Test function for visiting backward graph\n",
    "visited = set()\n",
    "def visit_all(var, indent=''):\n",
    "    if hasattr(var, 'data'):\n",
    "        print(\"{}Data: {}\".format(indent, var.data.size()))\n",
    "    if hasattr(var, 'grad'):\n",
    "        print(\"{}Grad Data: {}\".format(indent, var.grad))\n",
    "    if torch.is_tensor(var):\n",
    "        print(\"{}Tensor {}\".format(indent, var.size()))\n",
    "    else:\n",
    "        print(\"{}Type: {}\".format(indent, type(var)))\n",
    "\n",
    "    if hasattr(var, 'variable'):\n",
    "        print(\"{}Variable: {}\".format(indent, var.variable.size()))\n",
    "        print(\"{}Data: {}\".format(indent, var.variable))\n",
    "        print(\"{}Data GRAD: {}\".format(indent, var.variable.grad))\n",
    "    if hasattr(var, 'saved_tensors'):\n",
    "        print('{}Saved tensors: {}'.format(indent, var.saved_tensors))\n",
    "    if hasattr(var, 'next_functions'):\n",
    "        print('{}Next functions: {}'.format(indent, var.next_functions))\n",
    "        for f in var.next_functions:\n",
    "            if f[0] is not None:\n",
    "                print(\"{}Function: {}\".format(indent, f))\n",
    "                visit_all(f[0], indent=indent+'  ')\n",
    "    if hasattr(var, 'grad_fn'):\n",
    "        print('{}Grad Fn: {}'.format(indent, var.grad_fn))\n",
    "        visit_all(var.grad_fn, indent=indent+'  ')\n",
    "    visited.add(var)\n",
    "    \n",
    "    \n",
    "# visit_all(y)\n",
    "\n",
    "# print(out.grad_fn.next_functions)\n",
    "\n",
    "# out.__dir__()\n",
    "# make_dot(out, params=dict(feature_extractor.named_parameters()))\n",
    "make_dot(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple test for model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple experiment to check backward graph state\n",
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "c = torch.rand(1, requires_grad=True)\n",
    "y = torch.log(a) * (b + torch.pow(c, 3))\n",
    "print('a = {}'.format(a))\n",
    "print('b = {}'.format(b))\n",
    "print('c = {}'.format(c))\n",
    "print('y = {}'.format(y))\n",
    "\n",
    "# visit_all(y)\n",
    "# print('--- BACKWARD ---')\n",
    "# y.backward()\n",
    "# visit_all(y)\n",
    "\n",
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to _checkpoints/20180728_125439_road02_128_e002.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint\n",
    "def save_checkpoint(model, optimizer, experiment_name='test', epoch=None):\n",
    "    tstr = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    fname = '{}_{}'.format(tstr, experiment_name)\n",
    "    if epoch is not None:\n",
    "        fname += '_e{:03d}'.format(epoch)\n",
    "    fname += '.pth.tar'\n",
    "    \n",
    "    checkpoints_dir = '_checkpoints'\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "    \n",
    "    fname_path = os.path.join(checkpoints_dir, fname)\n",
    "#     print('fname_path = {}'.format(fname_path))\n",
    "    \n",
    "    checkpoint_dict = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optim_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint_dict, fname_path)\n",
    "    \n",
    "    print('Model saved to {}'.format(fname_path))\n",
    "    \n",
    "\n",
    "save_checkpoint(model, optimizer, 'road02_128', n_epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
