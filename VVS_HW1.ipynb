{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoseNet Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from datasets.apolloscape import Apolloscape\n",
    "from utils.common import draw_poses\n",
    "from utils.common import draw_record\n",
    "from utils.common import imshow\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APOLLO_PATH = \"./data/apolloscape\"\n",
    "APOLLO_PATH = \"/home/yellow/VSS_HW1/self-localization-sample/self-localization-sample\"\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "# Resize data before using\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(130),\n",
    "    transforms.CenterCrop(125),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stereo = False\n",
    "shuffle = True\n",
    "\n",
    "train_record = 'Record001'\n",
    "train_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark\",\n",
    "                             transform=transform, record=train_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=True, cache_transform=True, stereo=stereo)\n",
    "val_record = 'Record013'\n",
    "val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark\",\n",
    "                             transform=transform, record=val_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=False, cache_transform=True, stereo=stereo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=shuffle) # batch_size = 75\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=shuffle) # batch_size = 75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Records Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show records with numbers of data points\n",
    "recs_num = val_dataset.get_records_counts()\n",
    "recs_num = sorted(recs_num.items(), key=lambda kv: kv[0], reverse=True)\n",
    "print(\"Records Val:\")\n",
    "print(\"\\n\".join([\"\\t{} - {}\".format(r[0], r[1]) for r in recs_num ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_images(batch_samples):\n",
    "    just_images = [torch.cat(x, dim=2) for x in zip(*batch_samples[0])]\n",
    "    return just_images\n",
    "\n",
    "\n",
    "train_dataloader_iter = iter(train_dataloader)\n",
    "train_batch = next(train_dataloader_iter)\n",
    "\n",
    "print('len(batch) = {}'.format(len(train_batch)))\n",
    "print('len(batch[0]) = {}'.format(len(train_batch[0])))\n",
    "print('len(batch[0][0]) = {}'.format(len(train_batch[0][0])))\n",
    "\n",
    "if stereo:\n",
    "    pose = train_batch[1][0][0]\n",
    "else:\n",
    "    pose = train_batch[1][0]\n",
    "    \n",
    "    \n",
    "print('batch_poses[0] = ', pose)\n",
    "\n",
    "npose = np.zeros(7)\n",
    "npose[:3] = pose.numpy()[:3] * train_dataset.poses_std[:3] + train_dataset.poses_mean[:3]\n",
    "npose[3:] = pose.numpy()[3:]\n",
    "pose = npose\n",
    "print('unnormalized pose = ', pose)\n",
    "\n",
    "if stereo:\n",
    "    images_col = collate_batch_images(train_batch)\n",
    "else:\n",
    "    images_col = train_batch[0]\n",
    "\n",
    "img_out = vutils.make_grid(images_col, nrow=1)\n",
    "imshow(img_out, title=\"Batch 0\", img_normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Train and Val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw datasets\n",
    "draw_record(train_dataset)\n",
    "plt.show()\n",
    "\n",
    "record_id = 'Record013'\n",
    "draw_record(val_dataset, record=record_id, img_normalized=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_extractor, num_features=128):\n",
    "        super(PoseNet, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        fc_in_features = self.feature_extractor.fc.in_features\n",
    "        self.feature_extractor.fc = torch.nn.Linear(fc_in_features, num_features)\n",
    "        \n",
    "        # Translation\n",
    "        self.fc_xyz = torch.nn.Linear(num_features, 3)\n",
    "        \n",
    "        # Rotation in quaternions\n",
    "        self.fc_quat = torch.nn.Linear(num_features, 4)\n",
    "        \n",
    "    def extract_features(self, x):\n",
    "        x_features = self.feature_extractor(x)\n",
    "        x_features = F.relu(x_features)\n",
    "        x_features = F.dropout(x_features, p=0.5, training=self.training)\n",
    "        return x_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if type(x) is list:\n",
    "            x_features = [self.extract_features(xi) for xi in x]\n",
    "            x_translations = [self.fc_xyz(xi) for xi in x_features]\n",
    "            x_rotations = [self.fc_quat(xi) for xi in x_features]\n",
    "            x_poses = [torch.cat((xt, xr), dim=1) for xt, xr in zip(x_translations, x_rotations)]  \n",
    "        elif torch.is_tensor(x):\n",
    "            x_features = self.extract_features(x)\n",
    "            x_translations = self.fc_xyz(x_features) \n",
    "            x_rotations = self.fc_quat(x_features)\n",
    "            x_poses = torch.cat((x_translations, x_rotations), dim=1)\n",
    "        \n",
    "        return x_poses\n",
    "\n",
    "\n",
    "# Create model\n",
    "feature_extractor = models.resnet18(pretrained=True)\n",
    "model = PoseNet(feature_extractor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)\n",
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoseNetCriterion and sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_imgs1 = torch.rand(10, 3, 250, 250)\n",
    "# test_imgs2 = torch.rand(10, 3, 250, 250)\n",
    "if stereo:\n",
    "    batch_images = [torch.rand(1, 3, 125, 125) for _ in range(2)]\n",
    "    batch_poses = [torch.rand(1, 7) for _ in range(2)]\n",
    "else:\n",
    "    batch_images = torch.rand(1, 3, 125, 125)\n",
    "    batch_poses = torch.rand(1, 7)\n",
    "    \n",
    "class PoseNetCriterion(nn.Module):\n",
    "    def __init__(self, stereo=True, beta = 200.0):\n",
    "        super(PoseNetCriterion, self).__init__()\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "        self.stereo = stereo\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: list(N x 7, N x 7) or N x 7 - prediction (xyz, quat)\n",
    "            y: list(N x 7, N x 7) or N x 7 - target (xyz, quat)\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        if self.stereo:\n",
    "            for i in range(2):\n",
    "                # Translation loss\n",
    "                loss += self.loss_fn(x[i][:, :3], y[i][:, :3])\n",
    "                # Rotation loss\n",
    "                loss += self.beta * self.loss_fn(x[i][:, 3:], y[i][:, 3:])\n",
    "                \n",
    "            # Normalize per image so we can compare stereo vs no-stereo mode\n",
    "            loss = loss / 2\n",
    "        else:\n",
    "            # Translation loss\n",
    "            loss += self.loss_fn(x[:, :3], y[:, :3])\n",
    "\n",
    "            # Rotation loss\n",
    "            loss += self.beta * self.loss_fn(x[:, 3:], y[:, 3:])\n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "out = model(batch_images)\n",
    "print('out = {}'.format(out))\n",
    "\n",
    "criterion = PoseNetCriterion(stereo=stereo)\n",
    "loss = criterion(out, batch_poses)\n",
    "print('loss = {}'.format(loss))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "class AverageMeter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, value, n=1):\n",
    "        self.val = value\n",
    "        self.count += n\n",
    "        self.sum += value * n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "# train function\n",
    "def train(train_loader, model, criterion, optimizer, epoch, max_epoch, train_loss = []):\n",
    "    \n",
    "    # switch model to training\n",
    "    model.train()\n",
    "    \n",
    "    log_freq = 10\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    for idx, (batch_images, batch_poses) in enumerate(train_loader):\n",
    "#         if idx < len(train_loader) - 1: continue\n",
    "        data_time = (time.time() - end)\n",
    "        \n",
    "        if type(batch_images) is list:\n",
    "            batch_images = [x.to(device) for x in batch_images]\n",
    "        else: \n",
    "            batch_images = batch_images.to(device)\n",
    "            \n",
    "        if type(batch_poses) is list:\n",
    "            batch_poses = [x.to(device) for x in batch_poses]\n",
    "        else:\n",
    "            batch_poses = batch_poses.to(device)\n",
    "        \n",
    "        out = model(batch_images)\n",
    "        loss = criterion(out, batch_poses)\n",
    "\n",
    "        \n",
    "        losses.update(loss, len(batch_images) * batch_images[0].size(0) if stereo else batch_images.size(0))\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time = (time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if log_freq != 0 and idx % log_freq == 0:\n",
    "            print('Epoch: [{}/{}]\\tBatch: [{}/{}]\\t'\n",
    "                  'Time: {batch_time:.3f}\\t'\n",
    "                  'Data Time: {data_time:.3f}\\t'\n",
    "                  'Loss: {losses.val:.3f}\\t'\n",
    "                  'Avg Loss: {losses.avg:.3f}\\t'.format(\n",
    "                   epoch + 1, max_epoch, idx + 1, len(train_loader),\n",
    "                   batch_time=batch_time, data_time=data_time, losses=losses))\n",
    "            \n",
    "    print('Epoch: [{}/{}]\\tTraining Loss: {:.3f}'.format(epoch, max_epoch, losses.avg))\n",
    "    writer.add_scalar('Train/Epoch Loss', losses.avg, epoch)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def validate(val_loader, model, criterion, epoch, val_loss = []):\n",
    "    \n",
    "    \n",
    "    log_freq = 0 # len(val_loader)\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    # set model to evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (batch_images, batch_poses) in enumerate(val_loader):\n",
    "            data_time = time.time() - end\n",
    "            \n",
    "            # if stereo mode than we have lists\n",
    "            if type(batch_images) is list:\n",
    "                batch_images = [x.to(device) for x in batch_images]\n",
    "            else: \n",
    "                batch_images = batch_images.to(device)\n",
    "\n",
    "            if type(batch_poses) is list:\n",
    "                batch_poses = [x.to(device) for x in batch_poses]\n",
    "            else:\n",
    "                batch_poses = batch_poses.to(device)\n",
    "\n",
    "            \n",
    "            # compute model output\n",
    "            out = model(batch_images)\n",
    "            loss = criterion(out, batch_poses)\n",
    "            \n",
    "            losses.update(loss, len(batch_images) * batch_images[0].size(0) if stereo else batch_images.size(0))\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            if log_freq != 0 and idx % log_freq == 0:\n",
    "                print('Val Epoch: {}\\t'\n",
    "                      'Time: {batch_time:.3f}\\t'\n",
    "                      'Data Time: {data_time:.3f}\\t'\n",
    "                      'Loss: {losses.val:.3f}\\t'\n",
    "                      'Avg Loss: {losses.avg:.3f}'.format(\n",
    "                       epoch + 1, batch_time=batch_time, data_time=data_time, losses=losses))\n",
    "                \n",
    "    print('Epoch: [{}]\\tValidation Loss: {:.3f}'.format(epoch, losses.avg))\n",
    "    writer.add_scalar('Validation/Epoch Loss', losses.avg, epoch)\n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh model & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "model = PoseNet(feature_extractor)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore from checkpoint\n",
    "# checkpoint_file = '_checkpoints/20180811_164448_road02_128_e010.pth.tar'\n",
    "# checkpoint_file = '_checkpoints/20211104_204735_zpark_18_e10001.pth.tar'\n",
    "\n",
    "# if 'checkpoint_file' in locals() and checkpoint_file is not None:\n",
    "#     if os.path.isfile(checkpoint_file):\n",
    "#         print('Loading from checkpoint: {}'.format(checkpoint_file))\n",
    "#         checkpoint = torch.load(checkpoint_file)\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "#         start_epoch = checkpoint['epoch']\n",
    "        \n",
    "\n",
    "# if 'start_epoch' not in locals():\n",
    "#     start_epoch = 0\n",
    "# n_epochs = start_epoch + 1\n",
    "# print('Epochs {} - {}'.format(start_epoch, n_epochs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if 'start_epoch' not in locals():\n",
    "#     start_epoch = 0\n",
    "\n",
    "# n_epochs = start_epoch + 2\n",
    "# print('Training ...')\n",
    "# for e in range(start_epoch, n_epochs):\n",
    "    \n",
    "#     train(train_dataloader, model, criterion, optimizer, e, n_epochs)\n",
    "    \n",
    "# #     print('Validation ...')\n",
    "#     validate(val_dataloader, model, criterion, e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize PoseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in feature_extractor.parameters():\n",
    "#     print(p.size())\n",
    "\n",
    "# Test function for visiting backward graph\n",
    "visited = set()\n",
    "def visit_all(var, indent=''):\n",
    "    if hasattr(var, 'data'):\n",
    "        print(\"{}Data: {}\".format(indent, var.data.size()))\n",
    "    if hasattr(var, 'grad'):\n",
    "        print(\"{}Grad Data: {}\".format(indent, var.grad))\n",
    "    if torch.is_tensor(var):\n",
    "        print(\"{}Tensor {}\".format(indent, var.size()))\n",
    "    else:\n",
    "        print(\"{}Type: {}\".format(indent, type(var)))\n",
    "\n",
    "    if hasattr(var, 'variable'):\n",
    "        print(\"{}Variable: {}\".format(indent, var.variable.size()))\n",
    "        print(\"{}Data: {}\".format(indent, var.variable))\n",
    "        print(\"{}Data GRAD: {}\".format(indent, var.variable.grad))\n",
    "    if hasattr(var, 'saved_tensors'):\n",
    "        print('{}Saved tensors: {}'.format(indent, var.saved_tensors))\n",
    "    if hasattr(var, 'next_functions'):\n",
    "        print('{}Next functions: {}'.format(indent, var.next_functions))\n",
    "        for f in var.next_functions:\n",
    "            if f[0] is not None:\n",
    "                print(\"{}Function: {}\".format(indent, f))\n",
    "                visit_all(f[0], indent=indent+'  ')\n",
    "    if hasattr(var, 'grad_fn'):\n",
    "        print('{}Grad Fn: {}'.format(indent, var.grad_fn))\n",
    "        visit_all(var.grad_fn, indent=indent+'  ')\n",
    "    visited.add(var)\n",
    "    \n",
    "    \n",
    "# visit_all(y)\n",
    "\n",
    "# print(out.grad_fn.next_functions)\n",
    "\n",
    "# out.__dir__()\n",
    "# make_dot(out, params=dict(feature_extractor.named_parameters()))\n",
    "out = model(batch_images.to(device))\n",
    "make_dot(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple test for model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple experiment to check backward graph state\n",
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "c = torch.rand(1, requires_grad=True)\n",
    "y = torch.log(a) * (b + torch.pow(c, 3))\n",
    "print('a = {}'.format(a))\n",
    "print('b = {}'.format(b))\n",
    "print('c = {}'.format(c))\n",
    "print('y = {}'.format(y))\n",
    "\n",
    "# visit_all(y)\n",
    "# print('--- BACKWARD ---')\n",
    "# y.backward()\n",
    "# visit_all(y)\n",
    "\n",
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "def save_checkpoint(model, optimizer, experiment_name='test', epoch=None):\n",
    "    tstr = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    fname = '{}_{}'.format(tstr, experiment_name)\n",
    "    if epoch is not None:\n",
    "        fname += '_e{:03d}'.format(epoch)\n",
    "    fname += '.pth.tar'\n",
    "    \n",
    "    checkpoints_dir = '_checkpoints'\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "    \n",
    "    fname_path = os.path.join(checkpoints_dir, fname)\n",
    "#     print('fname_path = {}'.format(fname_path))\n",
    "    \n",
    "    checkpoint_dict = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optim_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint_dict, fname_path)\n",
    "    \n",
    "    print('Model saved to {}'.format(fname_path))\n",
    "    \n",
    "\n",
    "# save_checkpoint(model, optimizer, 'zpark_18', n_epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Error on Validation and Train Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate translation and rotation error of the predicted poses on train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import draw_poses\n",
    "from utils.common import draw_record\n",
    "from utils.common import imshow\n",
    "from utils.common import save_checkpoint\n",
    "from utils.common import AverageMeter\n",
    "from utils.common import calc_poses_params, quaternion_angular_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results_pred_gt(model, dataloader, poses_mean, poses_std, stereo=True):\n",
    "    model.eval()\n",
    "\n",
    "    gt_poses = np.empty((0, 7))\n",
    "    pred_poses = np.empty((0, 7))\n",
    "\n",
    "    for idx, (batch_images, batch_poses) in enumerate(dataloader):\n",
    "        \n",
    "        if stereo:\n",
    "            batch_images = [x.to(device) for x in batch_images]\n",
    "            batch_poses = [x.to(device) for x in batch_poses]\n",
    "        else:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_poses = batch_poses.to(device)\n",
    "\n",
    "\n",
    "        out = model(batch_images)\n",
    "        \n",
    "        loss = criterion(out, batch_poses)\n",
    "#         print('loss = {}'.format(loss))\n",
    "\n",
    "        # move data to cpu & numpy\n",
    "        if stereo:\n",
    "            batch_poses = [x.detach().cpu().numpy() for x in batch_poses]\n",
    "            out = [x.detach().cpu().numpy() for x in out]\n",
    "            gt_poses = np.vstack((gt_poses, *batch_poses))\n",
    "            pred_poses = np.vstack((pred_poses, *out))\n",
    "        else:\n",
    "            bp = batch_poses.detach().cpu().numpy()\n",
    "            outp = out.detach().cpu().numpy()\n",
    "            gt_poses = np.vstack((gt_poses, bp))\n",
    "            pred_poses = np.vstack((pred_poses, outp))\n",
    "\n",
    "\n",
    "        \n",
    "    # un-normalize translation\n",
    "    gt_poses[:, :3] = gt_poses[:, :3] * poses_std + poses_mean\n",
    "    pred_poses[:, :3] = pred_poses[:, :3] * poses_std + poses_mean\n",
    "    \n",
    "    return pred_poses, gt_poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Prediction and Ground Truth Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw ground truth in `blue` and predictions in `red` colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_3d_axes_limits(ax, poses, pose_format='quat'):\n",
    "    p_min, p_max, p_mean, p_std = calc_poses_params(poses, pose_format=pose_format)\n",
    "    ax.set_xlim(p_min[0], p_max[0])\n",
    "    ax.set_ylim(p_min[1], p_max[1])\n",
    "    ax.set_zlim(int(p_min[2] - 1), p_max[2])\n",
    "    return p_min, p_max, p_mean, p_std\n",
    "\n",
    "def draw_pred_gt_poses(pred_poses, gt_poses):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.set_xlabel('$X$')\n",
    "    ax.set_ylabel('$Y$')\n",
    "    ax.set_zlabel('$Z$')\n",
    "    ax.view_init(50, 30)\n",
    "\n",
    "    all_poses = np.concatenate((pred_poses, gt_poses))\n",
    "    p_min, _, _, _ = set_3d_axes_limits(ax, all_poses, pose_format='quat')\n",
    "    \n",
    "    draw_poses(ax, pred_poses[:, :3], proj=False, proj_z=int(p_min[2] - 1), c='r', s=60)\n",
    "    draw_poses(ax, gt_poses[:, :3], proj=False, proj_z=int(p_min[2] - 1), c='b', s=60)\n",
    "    for i in range(pred_poses.shape[0]):\n",
    "        pp = pred_poses[i, :3]\n",
    "        gp = gt_poses[i, :3]\n",
    "        pps = np.vstack((pp, gp))\n",
    "        ax.plot(pps[:, 0], pps[:, 1], pps[:, 2], c=(0.7, 0.7, 0.7, 0.4))\n",
    "        \n",
    "#     plt.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "n_epochs = start_epoch + 10000\n",
    "\n",
    "# batch_images.to(device)\n",
    "criterion.to(device)\n",
    "for e in range(start_epoch, n_epochs):\n",
    "    train(train_dataloader, model, criterion, optimizer, e, n_epochs)\n",
    "    validate(val_dataloader, model, criterion, e)\n",
    "\n",
    "    if e%10 == 0:\n",
    "        # Get mean and std from dataset\n",
    "        poses_mean = val_dataset.poses_mean\n",
    "        poses_std = val_dataset.poses_std\n",
    "\n",
    "        pred_poses, gt_poses = model_results_pred_gt(model, train_dataloader, poses_mean, poses_std, stereo=stereo)\n",
    "        # Save for later visualization\n",
    "        pred_poses_train = pred_poses\n",
    "        gt_poses_train = gt_poses\n",
    "\n",
    "\n",
    "        pred_poses, gt_poses = model_results_pred_gt(model, val_dataloader, poses_mean, poses_std, stereo=stereo)\n",
    "        # Save for later visualization\n",
    "        pred_poses_val = pred_poses\n",
    "        gt_poses_val = gt_poses\n",
    "        \n",
    "        \n",
    "        # draw\n",
    "        draw_pred_gt_poses(pred_poses_train, gt_poses_train)\n",
    "        plt.title('PoseNet on Train Dataset (epoch '+str(e)+')')\n",
    "#         plt.show()\n",
    "        plt.savefig('figure/train/'+str(e)+'.png')\n",
    "\n",
    "        draw_pred_gt_poses(pred_poses_val, gt_poses_val)\n",
    "        plt.title('PoseNet on Validation Dataset (epoch'+str(e)+')')\n",
    "#         plt.show()\n",
    "        plt.savefig('figure/validation/'+str(e)+'.png')\n",
    "\n",
    "print(\"Done traning\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_record = 'Record013'\n",
    "val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark\",\n",
    "                             transform=transform, record=val_record, normalize_poses=True,\n",
    "                             pose_format='quat', train=False, cache_transform=True, stereo=stereo)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=shuffle) # batch_size = 75\n",
    "\n",
    "print(val_dataset)\n",
    "# Get mean and std from dataset\n",
    "poses_mean = val_dataset.poses_mean\n",
    "poses_std = val_dataset.poses_std\n",
    "pred_poses, gt_poses = model_results_pred_gt(model, val_dataloader, poses_mean, poses_std, stereo=stereo)\n",
    "# Save for later visualization\n",
    "pred_poses_val = pred_poses\n",
    "gt_poses_val = gt_poses\n",
    "\n",
    "\n",
    "# draw\n",
    "draw_pred_gt_poses(pred_poses_val, gt_poses_val)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Get mean and std from dataset\n",
    "poses_mean = val_dataset.poses_mean\n",
    "poses_std = val_dataset.poses_std\n",
    "\n",
    "\n",
    "print('\\n=== Test Training Dataset ======')\n",
    "pred_poses, gt_poses = model_results_pred_gt(model, train_dataloader, poses_mean, poses_std, stereo=stereo)\n",
    "\n",
    "print('gt_poses = {}'.format(gt_poses.shape))\n",
    "print('pred_poses = {}'.format(pred_poses.shape))\n",
    "t_loss = np.asarray([np.linalg.norm(p - t) for p, t in zip(pred_poses[:, :3], gt_poses[:, :3])])\n",
    "q_loss = np.asarray([quaternion_angular_error(p, t) for p, t in zip(pred_poses[:, 3:], gt_poses[:, 3:])])\n",
    "\n",
    "print('poses_std = {:.3f}'.format(np.linalg.norm(poses_std)))\n",
    "print('Translation(T) error in meters and Rotation(R) error in degrees:')\n",
    "print('T: median = {:.3f}, mean = {:.3f}'.format(np.median(t_loss), np.mean(t_loss)))\n",
    "print('R: median = {:.3f}, mean = {:.3f}'.format(np.median(q_loss), np.mean(q_loss)))\n",
    "\n",
    "# Save for later visualization\n",
    "pred_poses_train = pred_poses\n",
    "gt_poses_train = gt_poses\n",
    "print('\\n=== Test Validation Dataset ======')\n",
    "pred_poses, gt_poses = model_results_pred_gt(model, val_dataloader, poses_mean, poses_std, stereo=stereo)\n",
    "\n",
    "print('gt_poses = {}'.format(gt_poses.shape))\n",
    "print('pred_poses = {}'.format(pred_poses.shape))\n",
    "t_loss = np.asarray([np.linalg.norm(p - t) for p, t in zip(pred_poses[:, :3], gt_poses[:, :3])])\n",
    "q_loss = np.asarray([quaternion_angular_error(p, t) for p, t in zip(pred_poses[:, 3:], gt_poses[:, 3:])])\n",
    "\n",
    "print('poses_std = {:.3f}'.format(np.linalg.norm(poses_std)))\n",
    "print('Translation(T) error in meters and Rotation(R) error in degrees:')\n",
    "print('T: median = {:.3f}, mean = {:.3f}'.format(np.median(t_loss), np.mean(t_loss)))\n",
    "print('R: median = {:.3f}, mean = {:.3f}'.format(np.median(q_loss), np.mean(q_loss)))\n",
    "\n",
    "# Save for later visualization\n",
    "pred_poses_val = pred_poses\n",
    "gt_poses_val = gt_poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
